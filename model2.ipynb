{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from PIL import Image\n",
    "import sys\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=6500)])\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import movie_images_to_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainning_folder=\"trainning_images\"\n",
    "test_folder=\"test_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_trainning_videos=3\n",
    "number_of_test_videos=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainning_data_set=[]\n",
    "test_data_set=[]\n",
    "# Loop through trainning images\n",
    "for index in range(number_of_trainning_videos):\n",
    "    trainning_data_set.append(pd.DataFrame(movie_images_to_dict.get_images_to_dict(f'{trainning_folder}/{index}/')).to_numpy())\n",
    "# Loop through test images\n",
    "for index in range(number_of_test_videos):\n",
    "    test_data_set.append(pd.DataFrame(movie_images_to_dict.get_images_to_dict(f'test_images/{index}/')).to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(trainning_data_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in to groups\n",
    "def split_in_groups(dataset,number_of_images=2,step=1):\n",
    "#     Generate in groups\n",
    "    dataset_gen=[dataset[i : i + number_of_images] for i in range(0, len(dataset), step) if i+number_of_images<len(dataset)]\n",
    "#     Return as a numpy group\n",
    "    return np.array(dataset_gen)\n",
    "\n",
    "\n",
    "# Trainning groups\n",
    "trainning_groups=[]\n",
    "for i in range(number_of_trainning_videos):\n",
    "    trainning_groups.append(split_in_groups(trainning_data_set[i],6))\n",
    "trainning_groups=np.concatenate(trainning_groups,axis=0)                            \n",
    "                            \n",
    "\n",
    "# Test groups\n",
    "test_groups=[]\n",
    "for i in range(number_of_test_videos):\n",
    "    test_groups.append(split_in_groups(test_data_set[i],6))                \n",
    "test_groups=np.concatenate(test_groups,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape=(int(800/8),int(1920/8),3)\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_and_rescale = tf.keras.Sequential([\n",
    "  K.layers.experimental.preprocessing.Resizing(input_shape[0],input_shape[1]),\n",
    "  K.layers.experimental.preprocessing.Rescaling(1./255)\n",
    "])\n",
    "def resize_image(images,y):\n",
    "    new_images=[]\n",
    "    for image in images:\n",
    "        new_images.append(resize_and_rescale(image))\n",
    "    print(new_images)\n",
    "    return tuple(new_images),y\n",
    "    \n",
    "# Gen\n",
    "def trainning_group_gen():\n",
    "    for trainning_group in trainning_groups:\n",
    "        images=[]\n",
    "        for record in trainning_group:\n",
    "            image=Image.open(record[1]+record[0])\n",
    "            image=image.resize((input_shape[1],input_shape[0]))\n",
    "            images.append(np.array(image)/255)\n",
    "\n",
    "        yield tuple(images),1\n",
    "        yield tuple(images[::-1]),0\n",
    "        \n",
    "def trainning_group_gen2():\n",
    "    for trainning_group in trainning_groups:\n",
    "        images=[]\n",
    "        for record in trainning_group:\n",
    "            image=Image.open(record[1]+record[0])\n",
    "            images.append(np.array(image))\n",
    "\n",
    "        yield tuple(images),1\n",
    "        yield tuple(images[::-1]),0\n",
    "        \n",
    "def test_group_gen():\n",
    "    for test_group in test_groups:\n",
    "        images=[]\n",
    "        for record in test_group:\n",
    "            image=Image.open(record[1]+record[0])\n",
    "            image=image.resize((input_shape[1],input_shape[0]))\n",
    "            images.append(np.array(image)/255)\n",
    "        yield tuple(images),1\n",
    "        yield tuple(images[::-1]),0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_trainning_dataset=tf.data.Dataset.from_generator(\n",
    "    trainning_group_gen,\n",
    "    output_signature=(\n",
    "    (tf.TensorSpec(shape=input_shape, dtype=tf.float32),tf.TensorSpec(shape=input_shape, dtype=tf.float32)),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.int8)\n",
    "    )\n",
    ").batch(20).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "tensor_trainning_dataset2=tf.data.Dataset.from_generator(\n",
    "    trainning_group_gen2,\n",
    "    output_signature=(\n",
    "    (tf.TensorSpec(shape=(800,1920,3), dtype=tf.float32),tf.TensorSpec(shape=(800,1920,3), dtype=tf.float32),tf.TensorSpec(shape=(800,1920,3), dtype=tf.float32),tf.TensorSpec(shape=(800,1920,3), dtype=tf.float32),tf.TensorSpec(shape=(800,1920,3), dtype=tf.float32),tf.TensorSpec(shape=(800,1920,3), dtype=tf.float32)),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.int8)\n",
    "    )\n",
    ").batch(20).map(resize_image, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "tensor_test_dataset=tf.data.Dataset.from_generator(\n",
    "    test_group_gen,\n",
    "    output_signature=(\n",
    "    (tf.TensorSpec(shape=input_shape, dtype=tf.float32),tf.TensorSpec(shape=input_shape, dtype=tf.float32),tf.TensorSpec(shape=input_shape, dtype=tf.float32),tf.TensorSpec(shape=input_shape, dtype=tf.float32),tf.TensorSpec(shape=input_shape, dtype=tf.float32),tf.TensorSpec(shape=input_shape, dtype=tf.float32)),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.int8)\n",
    "    )\n",
    ").batch(3).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.math.reduce_mean(np.array([[1,2,3],[2,3,4]],dtype=np.float32),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def block(block,num_filters):\n",
    "    block=K.layers.Conv2D(num_filters,3,padding='same')(block)\n",
    "    block=K.layers.LeakyReLU()(block)\n",
    "    block=K.layers.BatchNormalization()(block)\n",
    "    block=K.layers.Dropout(.1)(block)\n",
    "    block=K.layers.MaxPool2D((3,3))(block)\n",
    "    return block\n",
    "start=K.layers.Input(shape=input_shape)\n",
    "block_1=block(start,64)\n",
    "block_2=block(block_1,128)\n",
    "block_3=block(block_2,256)\n",
    "block_4=block(block_3,512)\n",
    "global_max=K.layers.GlobalMaxPool2D()(block_4)\n",
    "conv=K.Model(inputs=start,outputs=global_max)\n",
    "\n",
    "\n",
    "image_1=K.Input(input_shape)\n",
    "image_2=K.Input(input_shape)\n",
    "image_3=K.Input(input_shape)\n",
    "image_4=K.Input(input_shape)\n",
    "image_5=K.Input(input_shape)\n",
    "image_6=K.Input(input_shape)\n",
    "\n",
    "image_1_nn=conv(image_1)\n",
    "image_2_nn=conv(image_2)\n",
    "image_3_nn=conv(image_3)\n",
    "image_4_nn=conv(image_4)\n",
    "image_5_nn=conv(image_5)\n",
    "image_6_nn=conv(image_6)\n",
    "\n",
    "def add(images):\n",
    "    return images[0]+images[1]+images[2]+images[3]+images[4]+images[5]\n",
    "\n",
    "lambda_layer=K.layers.Lambda(add)([image_1_nn,image_2_nn,image_3_nn,image_4_nn,image_5_nn,image_6_nn])\n",
    "fc_1=K.layers.Dense(200)(lambda_layer)\n",
    "fc_1=K.layers.LeakyReLU()(fc_1)\n",
    "fc_1=K.layers.Dense(200)(fc_1)\n",
    "fc_1=K.layers.LeakyReLU()(fc_1)\n",
    "fc_2=K.layers.Dense(1)(fc_1)\n",
    "bce = tf.keras.losses.BinaryCrossentropy()\n",
    "model=K.Model(inputs=[image_1,image_2,image_3,image_4,image_5,image_6],outputs=fc_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "K.utils.plot_model(model,show_shapes=True,expand_nested=True,show_dtype=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_opts = tf.RunOptions(report_tensor_allocations_upon_oom = True)\n",
    "model.compile(\n",
    "#     options = run_opts,\n",
    "    loss=K.losses.BinaryCrossentropy(),\n",
    "    optimizer=K.optimizers.Adam(learning_rate=0.01),\n",
    "    metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(tensor_trainning_dataset2,epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}