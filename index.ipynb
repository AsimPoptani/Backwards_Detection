{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "10f1e1fd75c27a141a87cb3016ddd46fdf80c31e96a8b17ff8040b8e0249e7ce"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from PIL import Image\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import movie_images_to_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainning_folder=\"trainning_images\"\n",
    "test_folder=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainning_data_set=[]\n",
    "test_data_set=[]\n",
    "# Loop through trainning images\n",
    "for index in range(3):\n",
    "    trainning_data_set.append(pd.DataFrame(movie_images_to_dict.get_images_to_dict(f'/media/Data/backwards_detection/trainning_images/{index}/')).to_numpy())\n",
    "# Loop through test images\n",
    "for index in range(1):\n",
    "    test_data_set.append(pd.DataFrame(movie_images_to_dict.get_images_to_dict(f'/media/Data/backwards_detection/test_images/{index}/')).to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                   0                                               1    2  \\\n",
       "0       00000001.png  /media/Data/backwards_detection/test_images/0/  800   \n",
       "1       00000002.png  /media/Data/backwards_detection/test_images/0/  800   \n",
       "2       00000003.png  /media/Data/backwards_detection/test_images/0/  800   \n",
       "3       00000004.png  /media/Data/backwards_detection/test_images/0/  800   \n",
       "4       00000005.png  /media/Data/backwards_detection/test_images/0/  800   \n",
       "...              ...                                             ...  ...   \n",
       "149727  00149728.png  /media/Data/backwards_detection/test_images/0/  800   \n",
       "149728  00149729.png  /media/Data/backwards_detection/test_images/0/  800   \n",
       "149729  00149730.png  /media/Data/backwards_detection/test_images/0/  800   \n",
       "149730  00149731.png  /media/Data/backwards_detection/test_images/0/  800   \n",
       "149731  00149732.png  /media/Data/backwards_detection/test_images/0/  800   \n",
       "\n",
       "           3  \n",
       "0       1280  \n",
       "1       1280  \n",
       "2       1280  \n",
       "3       1280  \n",
       "4       1280  \n",
       "...      ...  \n",
       "149727  1280  \n",
       "149728  1280  \n",
       "149729  1280  \n",
       "149730  1280  \n",
       "149731  1280  \n",
       "\n",
       "[149732 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00000001.png</td>\n      <td>/media/Data/backwards_detection/test_images/0/</td>\n      <td>800</td>\n      <td>1280</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00000002.png</td>\n      <td>/media/Data/backwards_detection/test_images/0/</td>\n      <td>800</td>\n      <td>1280</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00000003.png</td>\n      <td>/media/Data/backwards_detection/test_images/0/</td>\n      <td>800</td>\n      <td>1280</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00000004.png</td>\n      <td>/media/Data/backwards_detection/test_images/0/</td>\n      <td>800</td>\n      <td>1280</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00000005.png</td>\n      <td>/media/Data/backwards_detection/test_images/0/</td>\n      <td>800</td>\n      <td>1280</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>149727</th>\n      <td>00149728.png</td>\n      <td>/media/Data/backwards_detection/test_images/0/</td>\n      <td>800</td>\n      <td>1280</td>\n    </tr>\n    <tr>\n      <th>149728</th>\n      <td>00149729.png</td>\n      <td>/media/Data/backwards_detection/test_images/0/</td>\n      <td>800</td>\n      <td>1280</td>\n    </tr>\n    <tr>\n      <th>149729</th>\n      <td>00149730.png</td>\n      <td>/media/Data/backwards_detection/test_images/0/</td>\n      <td>800</td>\n      <td>1280</td>\n    </tr>\n    <tr>\n      <th>149730</th>\n      <td>00149731.png</td>\n      <td>/media/Data/backwards_detection/test_images/0/</td>\n      <td>800</td>\n      <td>1280</td>\n    </tr>\n    <tr>\n      <th>149731</th>\n      <td>00149732.png</td>\n      <td>/media/Data/backwards_detection/test_images/0/</td>\n      <td>800</td>\n      <td>1280</td>\n    </tr>\n  </tbody>\n</table>\n<p>149732 rows Ã— 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "pd.DataFrame(test_data_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Union\n",
    "def shape(ndarray: Union[List, float]) -> Tuple[int, ...]:\n",
    "    if isinstance(ndarray, list):\n",
    "        # More dimensions, so make a recursive call\n",
    "        outermost_size = len(ndarray)\n",
    "        row_shape = shape(ndarray[0])\n",
    "        return (outermost_size, *row_shape)\n",
    "    else:\n",
    "        # No more dimensions, so we're done\n",
    "        return ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversion methods \n",
    "# will generate a array of x frames\n",
    "def every_x_frame(data,x=5):\n",
    "    size=data.shape[0]\n",
    "    left_over=size%x\n",
    "    n_times=math.floor(size/x)\n",
    "    data=data[:-left_over,:]\n",
    "    split=np.split(data,n_times)\n",
    "    return split\n",
    "\n",
    "def negative_frames(data,x):\n",
    "    data_to_be_shuffled=data.copy()\n",
    "    \n",
    "    np.random.shuffle(data_to_be_shuffled)\n",
    "    random=every_x_frame(data_to_be_shuffled,x)\n",
    "    flip=every_x_frame(np.flip(data,axis=0),x)\n",
    "    return flip+random\n",
    "\n",
    "# generates positive and negative tests for x frames\n",
    "def gen_every_x_frames(data,x=5):\n",
    "    data=data.copy()\n",
    "    positive=every_x_frame(data,x)\n",
    "    negative=negative_frames(data,x)\n",
    "    return [positive,negative]\n",
    "\n",
    "# generates positive and negative tests for x frames\n",
    "def gen_every_x_frames_skip(data,step=2,x=5):\n",
    "    data=data.copy()\n",
    "    data=data[::step]\n",
    "    return gen_every_x_frames(data,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trainning_data_set_gen():\n",
    "    data_set_pos=[]\n",
    "    data_set_neg=[]\n",
    "    for trainning in trainning_data_set:\n",
    "        # generate cases for 5 images and a skip of 2\n",
    "        positive, negative=gen_every_x_frames_skip(trainning)\n",
    "        data_set_pos=data_set_pos+positive\n",
    "        data_set_neg=data_set_neg+negative\n",
    "    return data_set_pos,data_set_neg\n",
    "def test_data_set_gen():\n",
    "    data_set_pos=[]\n",
    "    data_set_neg=[]\n",
    "    for test in test_data_set:\n",
    "        # generate cases for 5 images and a skip of 2\n",
    "        positive, negative=gen_every_x_frames_skip(test)\n",
    "        data_set_pos=data_set_pos+positive\n",
    "        data_set_neg=data_set_neg+negative\n",
    "    return data_set_pos,data_set_neg\n",
    "\n",
    "def pick_x_amount(data,x):\n",
    "    return random.sample(data,x)\n",
    "\n",
    "def open_images_prep_keras(posData, negData, limit, random=True):\n",
    "    # take positive and negative data and limit\n",
    "    if random:\n",
    "        posData=pick_x_amount(posData,limit)\n",
    "        negData=pick_x_amount(negData,limit)\n",
    "    else:\n",
    "        posData=posData[0:limit]\n",
    "        negData=negData[0:limit]\n",
    "    # image_paths Y\n",
    "    data_set=[]\n",
    "    for pos_images in posData:\n",
    "        image_paths=[]\n",
    "        for image in pos_images:\n",
    "            image_paths.append(image[1]+image[0])\n",
    "        xy=[image_paths,1]\n",
    "        data_set.append(xy)\n",
    "    for neg_images in negData:\n",
    "        image_paths=[]\n",
    "        for image in neg_images:\n",
    "            image_paths.append(image[1]+image[0])\n",
    "        xy=[image_paths,0]\n",
    "        data_set.append(xy)\n",
    "    data_set=np.array(data_set)\n",
    "    np.random.shuffle(data_set)\n",
    "    for data in data_set:\n",
    "        image_data=[]\n",
    "        for image_path in data[0]:\n",
    "            image_data.append(np.array(Image.open(image_path))/255)\n",
    "        yield np.array([np.array(image_data),np.array(data[1])])\n",
    "\n",
    "def trainning_images_gen():\n",
    "    keras_trainning_data_set_pos, keras_trainning_data_set_neg= trainning_data_set_gen()\n",
    "    return open_images_prep_keras(keras_trainning_data_set_pos,keras_trainning_data_set_neg,1)\n",
    "trainning_images_gen()\n",
    "\n",
    "def test_images_gen():\n",
    "    keras_test_data_set_pos, keras_test_data_set_neg= test_data_set_gen()\n",
    "    return open_images_prep_keras(keras_test_data_set_pos,keras_test_data_set_neg,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert input to output\n",
    "def start_of_cnn():\n",
    "    image_input=tf.keras.layers.Input(shape=(800,1920,3))\n",
    "    x=tf.keras.layers.Conv2D(filters=50,kernel_size=[3,3],activation='relu',padding='valid')(image_input)\n",
    "    x=tf.keras.layers.MaxPooling2D()(x)\n",
    "    x=tf.keras.layers.Dropout(.1)(x)\n",
    "    x=tf.keras.layers.Conv2D(filters=25,kernel_size=[3,3],activation='relu',padding='valid')(x)\n",
    "    x=tf.keras.layers.MaxPooling2D()(x)\n",
    "    x=tf.keras.layers.Dropout(.1)(x)\n",
    "    x=tf.keras.layers.Conv2D(filters=25,kernel_size=[3,3],activation='relu',padding='valid')(x)\n",
    "    x=tf.keras.layers.MaxPooling2D()(x)\n",
    "    x=tf.keras.layers.Dropout(.1)(x)\n",
    "    x=tf.keras.layers.Conv2D(filters=25,kernel_size=[3,3],activation='relu',padding='valid')(x)\n",
    "    x=tf.keras.layers.MaxPooling2D()(x)\n",
    "    x=tf.keras.layers.Dropout(.1)(x)\n",
    "    # x=tf.keras.Model(inputs=image_input,outputs=x) #(None, 48, 118, 25) \n",
    "    return image_input,x\n",
    "inputs=[]\n",
    "combine_layers=[]\n",
    "for _ in range(5):\n",
    "    \n",
    "combined=tf.keras.layers.concatenate(axis=1,inputs=[start_of_cnn(),start_of_cnn()])\n",
    "x=tf.keras.layers.Dense(200, activation=\"relu\")(combined)\n",
    "x=tf.keras.layers.Dense(200, activation=\"relu\")(x)\n",
    "x=tf.keras.layers.Dense(1, activation=\"softmax\")(x)\n",
    "# x=tf.keras.Model(inputs=image_input,outputs=x)"
   ]
  }
 ]
}